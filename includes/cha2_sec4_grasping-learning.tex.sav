\section{Robot grasping and manipulation by learning}
\label{cha2:sec4:grasping-learning}

%[El-Khoury and Sahbani, 2010].
%\citet{bekiroglu2011assessing} integral the information of the object shape, approach vector, tactile data and joint configuration to estimate a grasp quality.

% Why need to use learning, benefit?
As discussed in the Section~\ref{cha2:sec1:grasping}, conventional grasp and manipulation planning methods suffer from the curse of dimensionality.
To solve this problem, many new approaches have been proposed to avoid the complexity of computing kinematical constraints guaranteeing stable grasps by applying machine learning techniques.
In imitation learning, some researchers use datagloves for human demonstration. The human hand configuration is then mapped to an artificial hand workspace and the joint angles~\cite{Fischer98,Ekvall07}, or hand preshapes~\cite{Kyota05, Pelossof04, Li07} are learnt. Some other researchers use stereoscopy to track the hand when a demonstrator is performing a grasp~\cite{Hueser06} or to match the hand shape to a database of grasp images~\cite{Romero08}.
These learning based approaches succeed in taking into account the hand kinematics and generate hand preshapes that are compatible with the object features.

% Compare to analytical solution: more robust, but not guarantee
The learning approaches generate give precise contact point locations that guarantee grasp stability. Instead, most of them generate a grasp by less specific specifications such as the approaching vector and pre-grasp postures.
At the other hand, these learning approaches usually rely on statistical models. Therefor these approaches do not provide guarantee of the performance of the grasp, even if the plan is execute perfectly. The stability of the planned grasps can only be evaluated after execution. However, for the same reason they tolerate a certain amount of noise and are more robust to errors. Hence, these methods are more suited to human dominate environments. 

Demonstration based learning has been extensively studied~\citep{calinon2007learning,dillmann2004teaching,kulic2012incremental} as a promising approach to build robot intelligence. %It is essential for the tasks that analytical expression of the system is hard to derive.
Learning manipulation tasks is one of the main application of this approach. The physical properties of a manipulation task is hard to express analytically, and as a result the control strategy is hard to derive. Modeling expert's demonstration of strategies has been used as an alternative to the analytical solution.

Two major forms of demonstration are used in teaching manipulation tasks: kinematics teaching and tele-operation. In kinesthetic teaching, human directly contact with the robot and guide their movements to accomplish a task~\citep{korkinof2013online,pais2014encoding,pastor2011skill,Miao2014}. The trajectory of movements and contact force are recorded by the robot sensors.
% ===== Why not kinematics approach? =====
This method is simple and effective but limited in the number of controllable end effectors. While a manipulation task usually involves multifinger movement, a human can kinematically operate one finger with each hand and hence two fingers simultaneously at most. To control multi-finger hands, some researchers use tele-operation~\citep{bernardino2013precision,kondo2008recognition,Fischer98}. This usually relies on data gloves and motion capture system to sense human hand-arm motions. The human motion is mapped to robots to generate motions and interact with the environment. In fine manipulation tasks, the robot platforms are usually restricted to anthropomorphic hands for better mapping. All of these methods provide no direct force feedback to the human demonstrator during manipulation.

In some studies, the human demonstrate manipulation tasks with their own bodies~\citep{asfour2008imitation}. With direct interaction with the object the human demonstrator is able to perform the task most naturally and with a more delicate control strategy. The task information captured from these human demonstrations needs to be transferred to robots. Various mapping methods have been proposed~\citep{hueser2006learning,asfour2008imitation,do2011towards,}, while human correction~\citep{calinon2007incremental,sauser2011iterative,romano2011human} and self-correction via learning~\citep{bidan2013robio} are proposed as alternative solutions. In general, how to effectively transfer human skills to robots skill remains a challenge.



%machine learning techniques  to the grasping
%problem. Some researchers use datagloves, map human hand to artificial hand workspace and learn the
%different joint angles~\cite{Fischer98,Ekvall07}, or hand
%preshapes~\cite{Kyota05, Pelossof04, Li07}  in order to perform a grasp. Others use
%stereoscopy to track the demonstrator's hand performing a
%grasp~\cite{Hueser06} or try to recognize its hand shape from a
%database of grasp images~\cite{Romero08}.
%However they focus on different problems, such as telemanipulation~\cite{Fischer98} and human hand tracking~\cite{Hueser06}, rather than real time unattended grasping.
%Other group of researches concentrate on generating a list of grasps for one object~\citep{Kyota05, Pelossof04, Li07, saut2011efficient}.

