\chapter{Conclusion}
\label{cha7}

% What did I present
Throughout this dissertation, we explore the use of modular approaches in three subareas in robot grasping and manipulation: grasp planning, manipulation adaptive control and reaching motion planning. In our studies, the grasping and manipulation are formulated as learning problems. In learning, the difficulty caused by high dimension and non-linearity is tamed by modular approaches. 
%
%In this dissertation we discuss the application of modular approaches in robot grasping and manipulation. The basic principle is simple: the solution of a complicated task can be modeled better by a combination of a set of sub-solutions, each of which independently in charge of a subtask of the task. The sub-solutions are extracted from demonstrations and encoded with statistical models.
%This modular-learning hybrid approach is particularly useful for robot grasping and manipulation. Tasks involving in this category have frequently changing context and their system dynamics is hard to model analytically.
%In our method, while the imitation learning approach enable us to directly model the solutions without deeply analysing the system dynamics, the modular approach simplify the modeling process by dividing the big solution space to a set of smaller local regions. For frequently changing context, modular approaches have the advantage of being fast adaptive. Further, user-defined tasks is desired for service robots such as domestic robots. Our method provide an user-friendly framework to program robot, even for difficult adaptive control tasks. Different from many other studies on sequential motor primitives, our modular approaches are ``concurrent'': they are combined as a whole to provide solution for the task. Concurrent combination of modules provides more possibilities of solutions and hence is more feasible.


% ------------- Chapter 1 ---------------
In the chapter 1, we give an overview of the use of modular approaches in AI, control and robotics, and explain the motivation of using modular approaches in grasping and manipulation. We then further discuss the studies in the relative areas in the chapter 2. We particularly look into the state of art of modular approaches in robot grasping nd manipulation. 

In the chapter 3, we present our work in real time grasp planning. Two scenarios are considered: grasping known objects and grasping novel objects. For the first scenario, we generate training grasps for a given robot hand and a given object, and learn a GMM model of the stable grasp distribution. After the object is ``learnt'', new grasps can be quickly computed using the model. For the second scenario, we adopt a modular approach based on the concept of shape primitive. The novel objects are regarded as a combination of ``learnt'' shape primitives, and its grasp distribution is formed by combining the corresponding primitives' grasp distributions. Grasps for the novel object is then computed from the distribution. We implement this method on two different robot hands and show that the computation time is no more that 20 $msec$. This method enable the robot to react quickly in robot-human interaction and adapt to fast perturbations in a dynamic environment, and hence is suitable for a service robot. We show that modular approach can speed up solving high-dimensional planning problems. 

In the chapter 4, we explain our modular approach in a manipulation task and use it to implement a opening-bottle-task. 
%% ------------


% conclusion
In this chapter we present a modular approach for learning
manipulation tasks from human demonstration. We discover the number of
modules needed in a task by hierarchical clustering. From each cluster
we use forward and inverse model pairs to model the motor control
mechanism. The forward models predict the effect of the previous motor
command, while the inverse models compute a motor command to bring the
current state to a desired state. The statistical approach enables us
to estimate the reliability of the inferences of each module under the
current task context. The final motor command is the sum of the weighted
commands generated by each module. By exploiting an object-centric
viewpoint, the learnt human internal models can be easily transferred
to a robot. Our experiments verify that by this modular approach, the
robot can automatically recognize the current task context and compute
appropriate motor commands to accomplish a manipulation task, here opening
bottle caps.

% compare to others
Our approach is applicable to manipulation tasks that require adaptive
control strategies. It has a number of benefits compared to existing,
pervasive methods for adaptive control such as classic model
identification adaptive control and reinforcement
learning~\citep{narendra1995adaptation,khalil2004modeling,buchli2011learning}. Because we imitate human behaviors, we do not need to derive the
system dynamics nor the cost function of the tasks, which involve deep
insight into the task and can be painstaking. The difficulty of
modeling an adaptive strategy is further reduced by a modular
approach: dividing the large state space into several subspaces, where
the local strategies can be approximated more accurately. With this
approach, we divide a complex human strategy into a few modules, and
combine them to generate contextualized motor commands.

% object centric
Our object-centric approach is a practical approach for teaching a
robot manipulation tasks that require proprioception. This allows
human demonstration of the task with physical contact with the object,
which means the demonstrator can have direct feedback from their own
senses and perform the task naturally. We bypass the problem of direct
mapping of human movement and degrees of freedom to a robot's by
expressing the strategy from an object-centric viewpoint. This can
largely benefit learning manipulation tasks such as impedance control
task, as measuring human muscle impedance is hard while measuring the
impedance of an object is more feasible. This approach focuses on imitating object movement rather than human movement. For generating natural looking manipulation strategies, however, the object-centric approach does not guarantee good results.

% limitation: discontinuous
We compute the final motor command by summing the weighted output of
each module. This makes an assumption that the state space is
continuous. For tasks with discontinuous space, switching between different modules would be more applicable~\citep{narendra1995adaptation,nakanishi2013spatio}.


% summary
In summary, tasks involving multiple phases or different contexts are
hard to implement by a single model. A modular architecture is a
practical approach for both learning and controlling these tasks. As
manipulation usually involves multi-phase friction and multi-body
interaction, learning manipulation tasks with a modular approach can
simplify the modeling problem to a significant extent. We have
presented here a framework for training a modular model on observed
human demonstrations, discovering the strategies used by the humans
through a system of cluster analysis, and encoding
the results in generative models capable of driving robots. We have
demonstrated that we can use this framework to transfer strategies
used by a human to a robot, using the task of bottle-cap
opening. The demonstration showed not only `simple' transference from
human to robot, but the capacity for generalizing to similar but
previously-unobserved contexts, and to adapt sequences of actions in
response to the current context.
Different from the work presented in the last chapter, of which the modules are predefined by a few distinguishable shapes, the work presented in this chapter aim to solve the problem of extracting multiple strategies from human demonstration. This data driven approach is suitable for modularizing control strategies that the modules are not obviously distinguishable by intuition. This work presents the benefit of using a modular approach to simplify the problem of modeling a changing context task.


% -----------------------
\section{Conclusion}
\label{cha5:sec4}

% Motion primitive simplify
The system we present in this chapter uses the mimesis model to learn motion primitives for reaching motion. It provides an easy to use interface for both motion recognition and generation. Motion primitives are the elementary motions that accomplish basic functions. Recent brain research~\cite{bizzi2008combining} provides more evidence to support the hypothesis that, in order to reduce the degree of freedom, the vertebrate motor system generates motions by combining a small number of motor primitives. Our experiment shows that by combining two motion primitives (8 d.o.f) we can indeed generate different motion patterns. This framework simplifies the modelling of motion primitives.

% linguistic interface
From the functional point of view, the motion primitives form the vocabulary of motions. This naturally allows us to label motion primitives using words. In our system, each motion primitive is symbolized in the proto-symbol space and labelled by its effects, i.e. ``grasp high box", ``grasp low box" etc. This provides a linguistic interface for the human to instruct robot motion, by giving language instructions like ``go lower to grasp the box". By matching the words in the human instruction and the labels of the motion primitives, the robot will be able to adjust the mixing coefficient and generate new motions to execute the command.

% Webots
We implemented the system in the Webots simulator with the iCub robot. The interpolation of the proto-symbols produces new motion primitives. The correlation between the new motions and their effects are learnt using first order linear regression. In our future work of learning more complex motion primitives, higher order or non-linear regression may need to be employed.

%
The experiment presented here provides a good starting point for our future study in learning more motion primitives for object manipulation. It shows that it is possible to directly control the outcome of the motion pattern, without fine tuning different variables in the model. This has the advantage, from the users point of view, that planning the motion primitives can be achieved more intuitively.

In this chapter, we show the benefit of using a modular approach to generate new modules that can be defined by language. Here we exploit the modularity of language itself, which facilitates the designing and modulation of the modules. The combination of the modularity in motion planning and language builds a base for understanding between human and robot.

Together with the previous two chapters, we present three modular approaches with applications in robot grasping and manipulation. We show that modular approaches can simplify programming robot to do contact tasks. In the next chapter, we will further discuss the advantages of modular approaches, explain the limitations of our methods and point out a few possible directions of future studies.
%In this chapter, we show that with the mimesis model works effectively in generating motions various in one dimension (object size, object height). In the future work, we will further study the control in multiple dimensions, using interpolation between multi-proto-symbols.





% What did I contribute
