\chapter{Learning Motion Primitive for manipulation tasks}
\label{cha5}

\section{Introduction}
\label{cha5:sec1}
This chapter focus on learning motion primitives for manipulation tasks. In previous chapters, we focus on learning the strategy of dexterous manipulation including multi-finger grasp planing and adaptive force control strategy. These are done on the ``end effector level'' and rely on the robot limb to deliver the end effector to a proper position. In this chapter, we look into the whole body movement and study how to programm robot to achieve a proper posture for a desire task.

\paragraph{Motion primitive} ~\\
To accomplish a more complex task, a sequence of motion is needed. As discussed in the introduction chapter, the high dimensional searching space makes this sequence of motion difficult to generate. To reduce the searching space, the concept of motion primitives in neuroscience~\citep{bizzi2008combining} has been introduced to planning. The basic principle is to discretize a manipulation task into a set of motion primitives, that each serves for an elementary manipulation function. After modeling each primitive, the whole task then can be achieved by coordinating them properly.

Modeling motion primitives remains an open problem. Many literatures discuss how to design motion primitives that accomplish specific tasks ~\citep{michelman1994forming,felip2012manipulation,ijspeert2013dynamical}. In those works motion primitives are modeled as a set of differential equations or control rules. New motions are generated by tuning the parameters in the models. Deriving these equations and control policies is not an easy work, as well as fine tuning the parameters to generate new motions. These are in need of deep understanding of the task and the dynamic model. 

These difficulties can be alleviated by using the learning by demonstration approach and modeling the motion in the state space.. In this chapter we propose an easy to use system for learning manipulation motion primitives from human demonstration. To achieve this goal, we exploit the application of the mimesis model~\citep{inamura2004embodied} in learning motion primitives for object manipulation.

\begin{figure}
  \centering
  \includegraphics[width=6cm]{./fig_cha5/begin.jpg}
  \caption{ \scriptsize{iCub grasping a box by both arms}
}
    \label{begin}
    \vspace{-0.5cm}
\end{figure}



\paragraph{Mirror neurons and Mimesis Model} ~\\
Mimesis model is a mathematical realization of the function of the mirror neurons. Mirror neurons is a kind of neurons found in primates and birds, which fires both when the animal observe and execute a motion. In human brain, mirror neurons has show track in the area of the premotor cortex, the supplementary motor area, the primary somatosensory cortex and the inferior parietal cortex. These areas contribute to human control of motion and sensory reception. It is generally believed that mirror neurons is associated with animal's ability of learning by imitation and understanding action of others~\citep{rizzolatti2004mirror}. Motivated by this idea, many researchers try to understand the function of mirror neurons and hence implement it on robots to equip them with human level imitation and learning ability. We are also inspired by this idea and hence try to mimic the mechanism of mirror neuron to learn manipulation motion primitives.

Inamura and etc. develop the mimesis model that realize the functions of the mirror neurons: observe motion, recognize motion and generate motion.
This mimesis model has been shown to be effective in motion recognition, generation and robot coaching~\citep{inamura2008geometric,okuno2011motion}. It is built based on the Hidden Markov Model (HMM).
In the mimesis model, all demonstrated motion patterns are first encoded by a HMM. These HMMs are then projected to a topological space called ``proto-symbol space''. In this space, each HMM is projected as a point called ``proto-symbol'', and is labeled by the character of its representing motion, such as ``grasp low box" or ``grasp high box". The similarity between two motions (HMMs) are represented as the Euclidean distance between their proto-symbol.

Recognition of a unknown motion is done by projecting the unknown motion to the proto-symbol space. This give us a new proto-symbol. If the new proto-symbol is very close to a known proto-symbol, then it is very likely the unknown motion is the motion represented by the closest proto-symbol. At the other hand, new motion generation is done by exploring new proto-symbols, i.e. interpolate between the known proto-symbols. The generated new motions will be similar to but different from the motions encoded by the surrounding proto-symbols.

As we label each proto-symbol, the mimesis model provides a base of understanding of human and robot behavior. This even allow the robot user to adjust robot motion by natural language. For example, starting from the ``gasp low box" motion, we can instruct the robot to raise its arms higher to grasp a box on the top of a cabinet by the command ``not high enough, go higher to grasp". This command will generate a motion closer to the motion labeled by ``grasp high box".

Most of previous work of the mimesis model focus on learning whole body movement. Our work extend the mimesis model to learn motions of manipulation that involve interaction with objects.
The work of Kunori et al.~\citep{kunori2009associating} using hidden Markov models to encode motion primitives for object manipulation has a similar concept to our work. While they focused on extracting key features and reshaping movements for good performance, we focus on combining known manipulation motion primitives to generate new motions that can achieve the desired effects. Although interpolation of known motions is not new in motion synthesis~\citep{hoshino2004interpolation,glardon2004pca}, most of the existing work focus on free body motion. The application on object manipulation is rarely discussed.

The goal of this work is to develop an easy to use system for the robot to learn manipulation motion primitives and generate new motions to adapt to unseen scenarios. The system is implemented for a bimanual grasping task. Different from the static fingertip grasping synthesis~\ref{cha3}, in this task we focus on the grasp reaching motion.

\include{includes/cha5_sec2_method}
\include{includes/cha5_sec3_experiment}
\include{includes/cha5_sec4_discussion} 