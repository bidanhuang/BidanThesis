\chapter{Learning Motion Primitive for manipulation tasks}
\label{cha5}

\section{Introduction}
\label{cha5:sec1}
This chapter focuses on learning motion primitives for manipulation tasks. In previous chapters, we focus on learning the strategy of dexterous manipulation including multi-finger grasp planning and adaptive force control strategy. These are done at the ``end effector level'' and rely on the robot limb to deliver the end effector to a proper position. In this chapter, we look into the whole body movement and study how to program a robot to achieve a proper posture for a desired task.

\paragraph{Motion primitive} ~\\
To accomplish a more complex task, a sequence of motions is needed. As discussed in the introduction chapter, the high dimensional search space makes this sequence of motion difficult to generate. To reduce the search space, the concept of motion primitives in neuroscience~\citep{bizzi2008combining} has been introduced to planning. The basic principle is to discretize a manipulation task into a set of motion primitives, with each serving an elementary manipulation function. After modelling each primitive, the whole task then can be achieved by coordinating them properly.

Modelling motion primitives remains an open problem. Much literature discusses how to design motion primitives that accomplish specific tasks ~\citep{michelman1994forming,felip2012manipulation,ijspeert2013dynamical}. In those works motion primitives are modelled as a set of differential equations or control rules. New motions are generated by tuning the parameters in the models. Deriving these equations and control policies is not an easy task, neither is fine tuning the parameters to generate new motions. These activities require a deep understanding of the task and the dynamic model.

These difficulties can be alleviated by using the learning by demonstration approach and modelling the motion in state space. In this chapter we propose an easy to use system for learning manipulation motion primitives from human demonstration. To achieve this goal, we exploit the application of the mimesis model~\citep{inamura2004embodied} in learning motion primitives for object manipulation.

\begin{figure}
  \centering
  \includegraphics[width=6cm]{./fig_cha5/begin.jpg}
  \caption{ \scriptsize{iCub grasping a box with both arms}
}
    \label{begin}
    \vspace{-0.5cm}
\end{figure}



\paragraph{Mirror neurons and Mimesis Model} ~\\
The mimesis model is a mathematical realization of the function of the mirror neurons. Mirror neurons are a kind of neuron found in primates and birds, which fires both when the animals observe and execute a motion. In the human brain, mirror neurons has show trace
in the area of the premotor cortex, the supplementary motor area, the primary somatosensory cortex and the inferior parietal cortex. These areas contribute to human control of motion and sensory reception. It is generally believed that mirror neurons are associated with an animal's ability to learn by imitation and to understand the action of others~\citep{rizzolatti2004mirror}. Motivated by this idea, many researchers try to understand the function of mirror neurons and hence implement it on robots to equip them with human level imitation and learning ability. We are also inspired by this idea and hence try to mimic the mechanism of mirror neurons to learn manipulation motion primitives.

The mimesis model is developed to realize the functions of the mirror neurons: observe motion, recognize motion and generate motion.
It has been shown to be effective in motion recognition, generation and robot coaching~\citep{inamura2008geometric,okuno2011motion}. It is built based on the Hidden Markov Model (HMM).
In the mimesis model, all demonstrated motion patterns are first encoded by a HMM. These HMMs are then projected to a topological space called ``proto-symbol space''. In this space, each HMM is projected as a point called a ``proto-symbol'', and is labelled by the character of its representing motion, such as ``grasp low box" or ``grasp high box". The similarity between two motions (HMMs) is represented as the Euclidean distance between their proto-symbol.

Recognition of an unknown motion is achieved by projecting the unknown motion to the proto-symbol space. This gives us a new proto-symbol. If the new proto-symbol is very close to a known proto-symbol, then it is very likely the unknown motion is the motion represented by the closest proto-symbol. On the other hand, new motion generation is achieved by exploring new proto-symbols, i.e. interpolating between the known proto-symbols. The new motions generated will be similar to, but different from, the motions encoded by the surrounding proto-symbols.

As we label each proto-symbol, the mimesis model provides a base of understanding of human and robot behaviour. This even allows the robot user to adjust robot motion using natural language. For example, starting from the ``gasp low box" motion, we can instruct the robot to raise its arms higher to grasp a box on the top of a cabinet using the command ``not high enough, go higher to grasp". This command will generate a motion closer to the motion labelled by ``grasp high box".

Most of previous work of the mimesis model focuses on learning whole body movements. Our work extends the mimesis model to learn motions of manipulation that involve interaction with objects.
The work of Kunori et al.~\citep{kunori2009associating} using hidden Markov models to encode motion primitives for object manipulation has a similar concept to our work. While they focused on extracting key features and reshaping movements for good performance, we focus on combining known manipulation motion primitives to generate new motions that can achieve the desired effects. Although interpolation of known motions is not new in motion synthesis~\citep{hoshino2004interpolation,glardon2004pca}, most of the existing work focuses on free body motion. The application to object manipulation is rarely discussed.

The goal of this work is to develop an easy to use system for the robot to learn manipulation motion primitives and generate new motions to adapt to unseen scenarios. The system is implemented for a bi-manual grasping task. Different to the static fingertip grasping synthesis~\ref{cha3}, in this task we focus on the grasp reaching motion.

\include{includes/cha5_sec2_method}
\include{includes/cha5_sec3_experiment}
\include{includes/cha5_sec4_discussion}
