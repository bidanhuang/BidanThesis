\section{Discussion}
\label{cha4:sec4}

In this chapter we have presented a modular approach for learning
manipulation tasks from human demonstration. We discover the number of
modules needed in a task by hierarchical clustering. From each cluster
we use forward and inverse model pairs to model the motor control
mechanism. The forward models predict the effect of the previous motor
command, while the inverse models compute a motor command to bring the
current state to a desired state. The statistical approach enables us
to estimate the reliability of the inferences of each module under the
current task context. The final motor command is the sum of the weighted
commands generated by each module. By exploiting an object-centric
viewpoint, the learnt human internal models can be easily transferred
to a robot. Our experiments verify that by this modular approach, the
robot can automatically recognize the current task context and compute
proper motor commands to accomplish a manipulation task, here opening
bottle caps.


Our approach is applicable to manipulation tasks that require adaptive
control strategies. It has a number of benefits compared to existing,
pervasive methods for adaptive control such as classic model
identification adaptive control and reinforcement
learning~\citep{narendra1995adaptation,khalil2004modeling,buchli2011learning}. Because we imitate human behaviors, we do not need to derive the
system dynamics nor the cost function of the tasks, which involve deep
insight into the task and can be painstaking. The difficulty of
modeling an adaptive strategy is further reduced by a modular
approach: dividing the large state space into several subspaces, where
the local strategies can be approximated more accurately. With this
approach, we divide a complex human strategy into a few modules, and
combine them to generate contextualized motor commands.

Our object-centric approach is a practical approach for teaching a
robot manipulation tasks that require proprioception. This allows
human demonstration of the task with physical contact with the object,
which means the demonstrator can have direct feedback from their own
senses and perform the task naturally. We bypass the problem of direct
mapping of human movement and degrees of freedom to a robot's by
expressing the strategy from an object-centric viewpoint. This can
largely benefit learning manipulation tasks such as impedance control
task, as measuring human muscle impedance is hard while measuring the
impedance of an object is more feasible. This approach focus on imitating object movement rather than human movement. For generating natural looking manipulation strategies, however, the object-centric approach does not guarantee good results.

We compute the final motor command by summing the weighted output of
each module. This makes an assumption that the state space is
continuous. For tasks with discontinuous space, switching between different modules would be more applicable~\citep{narendra1995adaptation,nakanishi2013spatio}.

There are many promising directions of further studies extending the
work presented here. The first is to apply this approach to other
contact tasks and learn a more general human control strategy in
handling the instability caused by friction.
%Clarify the fact here that you hardly analyze the effect of changing the cap size and the positioning of the fingers on the cap which is revealed in the tactile signature, and that this will be future work.
In our study, we have focussed on the control strategy of unscrewing
the cap. We hardly analyzed the effect of changing the cap size and or
the positioning of the fingers on the cap, which is revealed in the
tactile signature. For the task here, these were not important and
did not cluster separately, but for other contexts these could be
important. We expect this analysis to advance the study of the task specific grasping
strategy~\citep{el2013generation,dang2014semantic} from the force prospective.

To extend our approach to learn tasks involving multiple steps, one
could also integrate this framework with task segmentation techniques,
to break down the task into atomic steps and recognize the steps
needed, still using an modular approach. However, we could expect this
to complicate the pint of module integration and require
better-informed action selection. 

In summary, tasks involving multiple phases or different contexts are
hard to implement by a single model. A modular architecture is a
practical approach for both learning and controlling these tasks. As
manipulation usually involves multi-phase friction and multi-body
interaction, learning manipulation tasks with a modular approach can
simplify the modeling problem to a significant extent. We have
presented here a framework for training a modular model on observed
human demonstrations, discovering the strategies used by the humans
through a system of cluster analysis, and encoding
the results in generative models capable of driving robots. We have
demonstrated that we can use this framework to transfer strategies
used by a human to a robot, using the task of bottle-cap
opening. The demonstration showed not only `simple' transference from
human to robot, but the capacity for generalizing to similar but
previously-unobserved contexts, and to adapt sequences of actions in
response to the current context.


%%TODO: Modularity comes from... benefit
