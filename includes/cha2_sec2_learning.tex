\section{A review of imitation learning}
\label{cha2:sec2}
%\label{cha2:sec2:learning}

This section provides a brief introduction to robot imitation learning and then reviews its applications in robot grasping and manipulation.

\subsection{Robot imitation learning}
\label{cha2:sec2:learning}
Since those first studies on robot imitation learning~\citep{hayes1994robot,friedrich1996robot}, this approach has become one of most popular research areas in robotics. It is considered to be a designer-friendly approach to teach robots new tasks. The aim of imitation learning, also referred to as ``Learning by Demonstration'' (LbD) or ``Programming by Demonstration'' (PbD) in some of the literatures, is to enable a robot to learn new skills by observing human demonstrations and then to reuse these skills in similar tasks. In recent years, this approach has been extensively studied~\citep{dillmann2004teaching,calinon2007learning,calinon2008robot,kulic2012incremental} as a promising approach to build robot intelligence.
\citet{argall2009survey} give one of the most recent review of literatures in this area.
%A recent survey give an overview of literatures in this area~\citep{argall2009survey}.

\subsection{Robot learning of grasping and manipulation}
\label{cha2:sec2:grasping-learning}
%\label{cha2:sec4:grasping-learning}

%[El-Khoury and Sahbani, 2010].
%\citet{bekiroglu2011assessing} integral the information of the object shape, approach vector, tactile data and joint configuration to estimate a grasp quality.

% Why need to use learning, benefit?
% ----- reduce complexity -----
As discussed in Section~\ref{cha2:sec1},
conventional grasp and manipulation planning methods suffer from the curse of dimensionality.
Learning techniques have been introduced to avoid the complexity of computing kinematic constraints guaranteeing stable grasps. Briefly speaking, robot grasping has two learning sources: imitation learning from human demonstration and learning from data collected from simulation.
In imitation learning, some researchers use datagloves for human demonstration. The human hand configuration is then mapped to an artificial hand workspace and the joint angles~\citep{Fischer1998,ekvall2007learning}, or hand preshapes~\citep{Kyota2005, pelossof2004svm, Li07} are learnt. Some other researchers use stereoscopy to track the hand when a demonstrator is performing a grasp~\citep{hueser2006learning} or to match the hand shape to a database of grasp images~\citep{Romero2008}. For long term automatic learning, markerless methods to track human hand and arm movements in the approach and grasp execution are studied~\citep{ekvall2007learning,do2009grasp}. These learning based approaches succeed in taking into account the hand kinematics and generate hand preshapes that are compatible with the object features.
Human grasp postures are usually mapped to robot hand postures in fixed schemes, according to the type of grasp chosen by humans.
To get around this mapping step, \citet{herzog2014learning} directly demonstrate grasps from a real robot hand. This method is relatively time consuming and hence it focuses on finding a way to maximize the use of the grasping experience, i.e. reuse the grasping strategies and compute grasps for novel objects.
Grasp demonstrations can also be generated in a virtual environment.
For a given object shape and a robot hand, thousands of good grasps can be easily generated in a simulator. With simulated demonstrations, \citet{pelossof2004svm} use a discriminative Support Vector Machine model to learn the correlation between the grasp configuration and grasp quality.
The work we present in the Chapter~\ref{cha3} also generate grasps from simulator. We use a generative Gaussian Mixture Model to learn the distribution of force closure grasps \citep{bidan2013grasp}. Both approaches are able to generate new grasps but our approach has the advantages of being able to compute stable grasps in real time for familiar and non-familiar objects.

To further reduce the complexity of the grasping problem, modular approaches are used. This will be discussed in the Section~\ref{cha2:sec3}.

% ----- uncertainty -----
Besides reducing the complexity of the grasping problem, learning approaches are also used to tackle those common problems that appear in the human environment: uncertainty and noise in perception data, novel objects and unforeseeable situations. Most of these learning approaches study how humans handle those situations and imitate the strategies. \citet{ekvall2007learning} and \citet{stulp2011learning} study human grasp motion and try to learn how humans choose the approach vector that is robust to noise in pose estimation. Driven by the same idea, the human grasp postures are also studied and mapped to robot hands~\citep{tegin2009demonstration}. Inaccurate execution of a grasp can also cause problems. Humans handle this issue by using tactile feedback.
With the recent advances in tactile sensing technology, many attempt to include the tactile sensory data in assessing the grasp stability.
After grasp execution, feedback from tactile sensors provide a more accurate estimation of grasp stability than what is provided by vision. This allows grasp correction and can avoid failed lifting of the object caused by unstable grasp~\citep{li2014learning}.
\citet{bekiroglu2011assessing} integrate the information of the object shape primitive, approach vector, tactile data and hand joint configuration to estimate a grasp quality.
In the later work, contact point locations are also taken into account~\citep{dang2012learning,dang2014stable}. The support vector machine (SVM) is the most used model in discriminating stable and instable grasps. These tactile based methods are also used to evaluate grasps of novel objects.

% ------ novel object ------
%The tactile feedback based methods usually do not rely on predefined object shapes. Hence these methods can be easily applied to estimate grasp quality for novel objects.
Human ability in generating grasps for novel objects is also studied and imitated.
\citet{detry2009learning} study the human Early-Cognitive-Vision (ECV), which includes colour and edge information that can be used to describe any objects. These features are associated with appropriate grasps and hence grasps of novel objects with matched features can be generated.
\citet{el2007learning} try to imitate the human mechanism of representing objects by segmenting objects into a set of superquadric shape primitives. The mechanism of a human choosing the grasp component is then learnt by a Neural Network~\citep{el2010new}.



% ----- fast adapt -----
The human environment is dynamic and full of perturbations. These perturbations cannot be foreseen and can only be handled when they happen. A learning approach is also used here to provide methods for quick adaptation. Methods are proposed to simplify the computation of grasps such that a moving object can be caught~\citep{harada2008fast,kim2012}. The work we present in the Chapter~\ref{cha3} shorten the computation time of a grasp to a few milliseconds.
Besides using visual features, tactile sensors can provide additional useful information not accessible by vision. Many methods for quick adaptation to the actual contact conditions are proposed~\citep{hsiao2010contact,hsiao2011robust,kazemi2012robust,sauser2011iterative,li2014learning}.




% --- Manipulation ---
%%% Compare to analytical solution: more robust, but not guarantee
%%The learning approaches generate give precise contact point locations that guarantee grasp stability. Instead, most of them generate a grasp by less specific specifications such as the approaching vector and pre-grasp postures.
%%At the other hand, these learning approaches usually rely on statistical models. Therefor these approaches do not provide guarantee of the performance of the grasp, even if the plan is execute perfectly. The stability of the planned grasps can only be evaluated after execution. However, for the same reason they tolerate a certain amount of noise and are more robust to errors. Hence, these methods are more suited to human dominate environments.
%%
%%Demonstration based learning has been extensively studied~\citep{calinon2007learning,dillmann2004teaching,kulic2012incremental} as a promising approach to build robot intelligence. %It is essential for the tasks that analytical expression of the system is hard to derive.
%%Learning manipulation tasks is one of the main application of this approach. The physical properties of a manipulation task is hard to express analytically, and as a result the control strategy is hard to derive. Modeling expert's demonstration of strategies has been used as an alternative to the analytical solution.
%%
%%Two major forms of demonstration are used in teaching manipulation tasks: kinematics teaching and tele-operation. In kinesthetic teaching, human directly contact with the robot and guide their movements to accomplish a task~\citep{korkinof2013online,pais2014encoding,pastor2011skill,Miao2014}. The trajectory of movements and contact force are recorded by the robot sensors.
%%% ===== Why not kinematics approach? =====
%%This method is simple and effective but limited in the number of controllable end effectors. While a manipulation task usually involves multifinger movement, a human can kinematically operate one finger with each hand and hence two fingers simultaneously at most. To control multi-finger hands, some researchers use tele-operation~\citep{bernardino2013precision,kondo2008recognition,Fischer98}. This usually relies on data gloves and motion capture system to sense human hand-arm motions. The human motion is mapped to robots to generate motions and interact with the environment. In fine manipulation tasks, the robot platforms are usually restricted to anthropomorphic hands for better mapping. All of these methods provide no direct force feedback to the human demonstrator during manipulation.
%%
%%In some studies, the human demonstrate manipulation tasks with their own bodies~\citep{asfour2008imitation}. With direct interaction with the object the human demonstrator is able to perform the task most naturally and with a more delicate control strategy. The task information captured from these human demonstrations needs to be transferred to robots. Various mapping methods have been proposed~\citep{hueser2006learning,asfour2008imitation,do2011towards,}, while human correction~\citep{calinon2007incremental,sauser2011iterative,romano2011human} and self-correction via learning~\citep{bidan2013robio} are proposed as alternative solutions. In general, how to effectively transfer human skills to robots skill remains a challenge.



%machine learning techniques  to the grasping
%problem. Some researchers use datagloves, map human hand to artificial hand workspace and learn the
%different joint angles~\citep{Fischer98,Ekvall07}, or hand
%preshapes~\citep{Kyota05, Pelossof04, Li07}  in order to perform a grasp. Others use
%stereoscopy to track the demonstrator's hand performing a
%grasp~\citep{Hueser06} or try to recognize its hand shape from a
%database of grasp images~\citep{Romero08}.
%However they focus on different problems, such as telemanipulation~\citep{Fischer98} and human hand tracking~\citep{Hueser06}, rather than real time unattended grasping.
%Other group of researches concentrate on generating a list of grasps for one object~\citep{Kyota05, Pelossof04, Li07, saut2011efficient}.

